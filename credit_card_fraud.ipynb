{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 666,
          "sourceType": "datasetVersion",
          "datasetId": 306
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Load and Prepare Data [4]**\n",
        "\n",
        "**Load the dataset.**\n",
        "\n",
        "**Artificially introduce MAR missing values (5-10%) in 2-3 numerical feature columns.**\n",
        "\n",
        "**Ensure the target variable is 'default payment next month'.**\n"
      ],
      "metadata": {
        "id": "iyzVjj_fmjTQ"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "uciml_default_of_credit_card_clients_dataset_path = kagglehub.dataset_download('uciml/default-of-credit-card-clients-dataset')\n",
        "data_path=os.path.join(uciml_default_of_credit_card_clients_dataset_path,'UCI_Credit_Card.csv')\n",
        "df_original = pd.read_csv(data_path)\n",
        "df = df_original.copy()\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "XbZa5iXAlVIB",
        "outputId": "607308ef-d373-43c6-ef34-d8eff494af46"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'default-of-credit-card-clients-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
              "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
              "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
              "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
              "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
              "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
              "\n",
              "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
              "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
              "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
              "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
              "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
              "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
              "\n",
              "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
              "0       0.0       0.0       0.0                           1  \n",
              "1    1000.0       0.0    2000.0                           1  \n",
              "2    1000.0    1000.0    5000.0                           0  \n",
              "3    1100.0    1069.0    1000.0                           0  \n",
              "4    9000.0     689.0     679.0                           0  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd9931ce-ca97-4952-8e68-f94efb1470f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>...</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "      <th>default.payment.next.month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>689.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>120000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3272.0</td>\n",
              "      <td>3455.0</td>\n",
              "      <td>3261.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>90000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>14331.0</td>\n",
              "      <td>14948.0</td>\n",
              "      <td>15549.0</td>\n",
              "      <td>1518.0</td>\n",
              "      <td>1500.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>28314.0</td>\n",
              "      <td>28959.0</td>\n",
              "      <td>29547.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>1100.0</td>\n",
              "      <td>1069.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20940.0</td>\n",
              "      <td>19146.0</td>\n",
              "      <td>19131.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>36681.0</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>9000.0</td>\n",
              "      <td>689.0</td>\n",
              "      <td>679.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd9931ce-ca97-4952-8e68-f94efb1470f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd9931ce-ca97-4952-8e68-f94efb1470f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd9931ce-ca97-4952-8e68-f94efb1470f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1e53a58b-be49-45d6-844b-84081a8e2acc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e53a58b-be49-45d6-844b-84081a8e2acc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1e53a58b-be49-45d6-844b-84081a8e2acc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 369
        }
      ],
      "execution_count": 369
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "J55mo6SYmVz6",
        "outputId": "571d452d-aa4f-4e57-f2fd-5b3ea7d34f13"
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                            0\n",
              "LIMIT_BAL                     0\n",
              "SEX                           0\n",
              "EDUCATION                     0\n",
              "MARRIAGE                      0\n",
              "AGE                           0\n",
              "PAY_0                         0\n",
              "PAY_2                         0\n",
              "PAY_3                         0\n",
              "PAY_4                         0\n",
              "PAY_5                         0\n",
              "PAY_6                         0\n",
              "BILL_AMT1                     0\n",
              "BILL_AMT2                     0\n",
              "BILL_AMT3                     0\n",
              "BILL_AMT4                     0\n",
              "BILL_AMT5                     0\n",
              "BILL_AMT6                     0\n",
              "PAY_AMT1                      0\n",
              "PAY_AMT2                      0\n",
              "PAY_AMT3                      0\n",
              "PAY_AMT4                      0\n",
              "PAY_AMT5                      0\n",
              "PAY_AMT6                      0\n",
              "default.payment.next.month    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SEX</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EDUCATION</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MARRIAGE</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BILL_AMT1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BILL_AMT2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BILL_AMT3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_AMT6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>default.payment.next.month</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 370
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical columns, excluding the target variable\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "target_variable = 'default.payment.next.month'\n",
        "numerical_cols.remove(target_variable)\n",
        "\n",
        "print(\"Numerical columns:\", numerical_cols)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Randomly select 2 to 3 numerical columns\n",
        "num_cols_to_impute = np.random.randint(2, 4)\n",
        "cols_to_impute = np.random.choice(numerical_cols, num_cols_to_impute, replace=False)\n",
        "\n",
        "# Determine the number of rows to introduce missing values (5-10%)\n",
        "percentage_missing = np.random.uniform(0.05, 0.10)\n",
        "num_rows_to_impute = int(len(df) * percentage_missing)\n",
        "\n",
        "print(f\"Selected columns for imputation: {cols_to_impute}\")\n",
        "print(f\"Number of rows to introduce missing values: {num_rows_to_impute}\")\n",
        "\n",
        "\n",
        "# Introduce missing values\n",
        "for col in cols_to_impute:\n",
        "    rows_to_impute = np.random.choice(df.index, num_rows_to_impute, replace=False)\n",
        "    df.loc[rows_to_impute, col] = np.nan\n",
        "\n",
        "\n",
        "# Verify missing values\n",
        "print(\"Missing values after imputation:\")\n",
        "display(df[cols_to_impute].isnull().sum())\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "UBBUDUNjmoC3",
        "outputId": "16744687-1862-4b36-d608-ea6c569b80bf"
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical columns: ['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
            "Selected columns for imputation: ['BILL_AMT4' 'PAY_2' 'SEX']\n",
            "Number of rows to introduce missing values: 1917\n",
            "Missing values after imputation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BILL_AMT4    1917\n",
              "PAY_2        1917\n",
              "SEX          1917\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <td>1917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PAY_2</th>\n",
              "      <td>1917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SEX</th>\n",
              "      <td>1917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Imputation Strategy 1: Simple Imputation (Baseline) [4]**\n",
        "\n",
        "**Create a clean dataset copy (Dataset A).**\n",
        "\n",
        "**For each column with missing values, fill the missing values with the median of that column.**\n",
        "\n",
        "**Explain why the median is often preferred over the mean for imputation.**\n",
        "\n",
        "\n",
        "\n",
        "The median is  preferred over the mean for imputation because it is more robust to outliers and skewed distributions. Unlike the mean, which can be heavily influenced by extreme values, the median represents the middle value of a dataset and remains stable even when outliers are present. This makes it a more reliable choice for imputation, especially in datasets where the distribution of values is not symmetrical or contains anomalies. Using the median helps prevent distortion of the data's central tendency, leading to more accurate and consistent results in downstream analysis or modeling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xv1uh-DCoRrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a clean copy of the original dataset\n",
        "dataset_A = df.copy()\n",
        "\n",
        "# Identify columns with missing values\n",
        "cols_with_missing = dataset_A.columns[dataset_A.isnull().any()].tolist()\n",
        "\n",
        "# Impute missing values with the median of each column\n",
        "for col in cols_with_missing:\n",
        "    median_value = dataset_A[col].median()\n",
        "    dataset_A[col].fillna(median_value, inplace=True)\n",
        "\n",
        "# Check if missing values remain\n",
        "print(\"Missing values after median imputation (Dataset A):\")\n",
        "print(dataset_A.isnull().sum())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GuNQUaRDo-GV",
        "outputId": "502e4ffc-4cba-495b-e400-d0ce32a7aefd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after median imputation (Dataset A):\n",
            "ID                            0\n",
            "LIMIT_BAL                     0\n",
            "SEX                           0\n",
            "EDUCATION                     0\n",
            "MARRIAGE                      0\n",
            "AGE                           0\n",
            "PAY_0                         0\n",
            "PAY_2                         0\n",
            "PAY_3                         0\n",
            "PAY_4                         0\n",
            "PAY_5                         0\n",
            "PAY_6                         0\n",
            "BILL_AMT1                     0\n",
            "BILL_AMT2                     0\n",
            "BILL_AMT3                     0\n",
            "BILL_AMT4                     0\n",
            "BILL_AMT5                     0\n",
            "BILL_AMT6                     0\n",
            "PAY_AMT1                      0\n",
            "PAY_AMT2                      0\n",
            "PAY_AMT3                      0\n",
            "PAY_AMT4                      0\n",
            "PAY_AMT5                      0\n",
            "PAY_AMT6                      0\n",
            "default.payment.next.month    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZWkvRGUYp3lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N-M-pDpRf6jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputation Strategy 2: Regression Imputation (Linear) [6]**\n",
        "\n",
        "**Create a second clean dataset copy (Dataset B).**\n",
        "\n",
        "**Select a single column (your choice) that contains missing values.**\n",
        "\n",
        "**Use a Linear Regression model to predict the missing values based on all other non-missing features.**\n",
        "\n",
        "**Explain the underlying assumption of this method (Missing At Random).**\n",
        "\n",
        "\n",
        "The underlying assumption of regression imputation is Missing At Random (MAR). This means that the probability of a value being missing depends only on other observed variables, not on the missing value itself. In other words, the missingness can be explained by the data we do have. For example, if income is missing more often for younger people, and we know their age, then the missing income values are considered MAR. This assumption allows us to build a regression model using the observed features to predict and impute the missing ones."
      ],
      "metadata": {
        "id": "-Jyoirbrf4tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Lyd3kNTrhQtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = df.columns[df.isnull().any()]\n",
        "missing_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-N5uwshKbh",
        "outputId": "4dd10a82-8355-4ea4-ff3e-ddb97b31b0c1"
      },
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SEX', 'PAY_2', 'BILL_AMT4'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 374
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = df.columns[df.isnull().any()]\n",
        "print(\"Columns with missing values:\", list(missing_cols))\n",
        "\n",
        "\n",
        "column_to_keep = \"BILL_AMT4\"\n",
        "\n",
        "# Replace all other columns with the original ones\n",
        "for col in missing_cols:\n",
        "    if col != column_to_keep:\n",
        "        df[col] = df_original[col]\n",
        "\n",
        "print(\"\\n Replacement complete!\")\n",
        "print(\"These columns were replaced:\", [c for c in missing_cols if c != column_to_keep])\n",
        "print(\"This column was kept as-is:\", column_to_keep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY7g3oiHtUgI",
        "outputId": "b7495ed5-fdf6-490c-ac40-5ea6af6561a6"
      },
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with missing values: ['SEX', 'PAY_2', 'BILL_AMT4']\n",
            "\n",
            " Replacement complete!\n",
            "These columns were replaced: ['SEX', 'PAY_2']\n",
            "This column was kept as-is: BILL_AMT4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def regression_imputation(dataset, col_to_impute=None, target_col='default.payment.next.month', ):\n",
        "\n",
        "    # Create a copy of the dataset\n",
        "    dataset_B = dataset.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    cols_with_missing_B = dataset_B.columns[dataset_B.isnull().any()].tolist()\n",
        "\n",
        "    if len(cols_with_missing_B) == 0:\n",
        "        print(\"No missing values found in the dataset!\")\n",
        "        return dataset_B, {}\n",
        "\n",
        "    # Choose column for regression imputation\n",
        "    if col_to_impute is None:\n",
        "        col_to_impute_regression = cols_with_missing_B[0]\n",
        "    else:\n",
        "        col_to_impute_regression = col_to_impute\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"REGRESSION IMPUTATION (LINEAR)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nColumn chosen for regression imputation: {col_to_impute_regression}\")\n",
        "    print(f\"Missing values before imputation: {dataset_B[col_to_impute_regression].isnull().sum()}\")\n",
        "    print(f\"Percentage missing: {dataset_B[col_to_impute_regression].isnull().sum() / len(dataset_B) * 100:.2f}%\")\n",
        "\n",
        "    # Store original values for comparison (where they exist)\n",
        "    original_values = dataset_B[col_to_impute_regression].copy()\n",
        "\n",
        "    # Create two datasets: one with missing values and one without\n",
        "    df_missing = dataset_B[dataset_B[col_to_impute_regression].isnull()].copy()\n",
        "    df_not_missing = dataset_B[~dataset_B[col_to_impute_regression].isnull()].copy()\n",
        "\n",
        "    # Define features (X) and target (y) for the regression model\n",
        "    features = [col for col in dataset_B.columns\n",
        "                if col != col_to_impute_regression and col != target_col]\n",
        "\n",
        "    print(f\"\\nNumber of features used for prediction: {len(features)}\")\n",
        "    print(f\"Training samples (non-missing): {len(df_not_missing)}\")\n",
        "    print(f\"Prediction samples (missing): {len(df_missing)}\")\n",
        "\n",
        "    X_train = df_not_missing[features].copy()\n",
        "    y_train = df_not_missing[col_to_impute_regression].copy()\n",
        "    X_predict = df_missing[features].copy()\n",
        "\n",
        "    # Handle potential missing values in feature columns\n",
        "    # Use median imputation for numerical features\n",
        "    imputed_features = []\n",
        "    for feature in features:\n",
        "        if X_train[feature].isnull().any():\n",
        "            if X_train[feature].dtype in ['float64', 'int64']:\n",
        "                median_val = X_train[feature].median()\n",
        "                X_train[feature].fillna(median_val, inplace=True)\n",
        "                X_predict[feature].fillna(median_val, inplace=True)\n",
        "                imputed_features.append(feature)\n",
        "            else:\n",
        "                # For categorical, use mode\n",
        "                mode_val = X_train[feature].mode()[0]\n",
        "                X_train[feature].fillna(mode_val, inplace=True)\n",
        "                X_predict[feature].fillna(mode_val, inplace=True)\n",
        "                imputed_features.append(feature)\n",
        "\n",
        "    if imputed_features:\n",
        "        print(f\"\\nFeatures that also had missing values (imputed with median/mode): {imputed_features}\")\n",
        "\n",
        "    # Train the Linear Regression model\n",
        "    print(\"\\nTraining Linear Regression model...\")\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate model performance on training data\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    r2 = r2_score(y_train, y_train_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "\n",
        "    print(f\"\\nModel Performance (on non-missing data):\")\n",
        "    print(f\"  R² Score: {r2:.4f}\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\")\n",
        "    print(f\"  Mean of original values: {y_train.mean():.4f}\")\n",
        "    print(f\"  Std of original values: {y_train.std():.4f}\")\n",
        "\n",
        "    # Predict the missing values\n",
        "    predicted_values = model.predict(X_predict)\n",
        "\n",
        "    print(f\"\\nPredicted values statistics:\")\n",
        "    print(f\"  Mean: {predicted_values.mean():.4f}\")\n",
        "    print(f\"  Std: {predicted_values.std():.4f}\")\n",
        "    print(f\"  Min: {predicted_values.min():.4f}\")\n",
        "    print(f\"  Max: {predicted_values.max():.4f}\")\n",
        "\n",
        "    # Impute the missing values in dataset_B\n",
        "    dataset_B.loc[dataset_B[col_to_impute_regression].isnull(), col_to_impute_regression] = predicted_values\n",
        "\n",
        "    print(f\"\\nMissing values after regression imputation: {dataset_B[col_to_impute_regression].isnull().sum()}\")\n",
        "\n",
        "    # Explanation of MAR assumption\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"UNDERLYING ASSUMPTION: Missing At Random (MAR)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "\n",
        "    return dataset_B\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "70QAWJ0Up51k"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_B= regression_imputation(dataset=df,col_to_impute='BILL_AMT4',target_col='default.payment.next.month')"
      ],
      "metadata": {
        "id": "SiKkVfwMsr-i",
        "outputId": "f29f3981-79f3-412d-d9fb-7ec83497fec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "REGRESSION IMPUTATION (LINEAR)\n",
            "================================================================================\n",
            "\n",
            "Column chosen for regression imputation: BILL_AMT4\n",
            "Missing values before imputation: 1917\n",
            "Percentage missing: 6.39%\n",
            "\n",
            "Number of features used for prediction: 23\n",
            "Training samples (non-missing): 28083\n",
            "Prediction samples (missing): 1917\n",
            "\n",
            "Training Linear Regression model...\n",
            "\n",
            "Model Performance (on non-missing data):\n",
            "  R² Score: 0.9511\n",
            "  RMSE: 14212.4288\n",
            "  Mean of original values: 43162.4754\n",
            "  Std of original values: 64257.8473\n",
            "\n",
            "Predicted values statistics:\n",
            "  Mean: 44970.8169\n",
            "  Std: 63707.7182\n",
            "  Min: -9012.1277\n",
            "  Max: 449540.8796\n",
            "\n",
            "Missing values after regression imputation: 0\n",
            "\n",
            "================================================================================\n",
            "UNDERLYING ASSUMPTION: Missing At Random (MAR)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Eh5TKwH-tj87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputation Strategy 3: Regression Imputation (Non-Linear) [6]**\n",
        "\n",
        "**Create a third clean dataset copy (Dataset C).**\n",
        "\n",
        "**For the same column as in Strategy 2, use a non-linear regression model (e.g., K-Nearest Neighbors Regression or Decision Tree Regression) to predict the missing values.**\n"
      ],
      "metadata": {
        "id": "-toY13CetoHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def nonlinear_regression_imputation_knn(dataset, col_to_impute=None,\n",
        "                                        target_col='default.payment.next.month',\n",
        "                                        n_neighbors=5):\n",
        "    \"\"\"\n",
        "    Perform Non-Linear Regression Imputation using K-Nearest Neighbors (KNN)\n",
        "    on the specified column of a dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_C = dataset.copy()\n",
        "    cols_with_missing = dataset_C.columns[dataset_C.isnull().any()].tolist()\n",
        "\n",
        "    if not cols_with_missing:\n",
        "        print(\"No missing values found in the dataset!\")\n",
        "        return dataset_C\n",
        "\n",
        "    if col_to_impute is None:\n",
        "        col_to_impute = cols_with_missing[0]\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"NON-LINEAR REGRESSION IMPUTATION USING KNN\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Column chosen for imputation: {col_to_impute}\")\n",
        "    print(f\"Missing values before imputation: {dataset_C[col_to_impute].isnull().sum()}\")\n",
        "\n",
        "    # Split data into missing and non-missing subsets\n",
        "    df_missing = dataset_C[dataset_C[col_to_impute].isnull()].copy()\n",
        "    df_not_missing = dataset_C[~dataset_C[col_to_impute].isnull()].copy()\n",
        "\n",
        "    features = [col for col in dataset_C.columns\n",
        "                if col not in [col_to_impute, target_col]]\n",
        "\n",
        "    X_train = df_not_missing[features].copy()\n",
        "    y_train = df_not_missing[col_to_impute].copy()\n",
        "    X_predict = df_missing[features].copy()\n",
        "\n",
        "    # Handle missing values in features by median/mode imputation\n",
        "    for feature in features:\n",
        "        if X_train[feature].isnull().any():\n",
        "            if X_train[feature].dtype in ['float64', 'int64']:\n",
        "                fill_val = X_train[feature].median()\n",
        "            else:\n",
        "                fill_val = X_train[feature].mode()[0]\n",
        "            X_train[feature].fillna(fill_val, inplace=True)\n",
        "            X_predict[feature].fillna(fill_val, inplace=True)\n",
        "\n",
        "    # Standardize features (important for KNN)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_predict_scaled = scaler.transform(X_predict)\n",
        "\n",
        "    # Train KNN model\n",
        "    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluate on training data\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    r2 = r2_score(y_train, y_train_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "    print(f\"\\nModel Performance on Non-Missing Data:\")\n",
        "    print(f\"  R² Score: {r2:.4f}\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\")\n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "\n",
        "    # Predict and impute missing values\n",
        "    predicted_values = model.predict(X_predict_scaled)\n",
        "    dataset_C.loc[dataset_C[col_to_impute].isnull(), col_to_impute] = predicted_values\n",
        "\n",
        "    print(f\"\\nMissing values after imputation: {dataset_C[col_to_impute].isnull().sum()}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "\n",
        "    return dataset_C\n"
      ],
      "metadata": {
        "id": "uwqWYAzwXOE6"
      },
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_C=nonlinear_regression_imputation_knn(df, col_to_impute=None,\n",
        "                                        target_col='default.payment.next.month',\n",
        "                                        n_neighbors=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4RntszsXViq",
        "outputId": "02ca22c7-1664-4d99-c458-82fc8f796dde"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "NON-LINEAR REGRESSION IMPUTATION USING KNN\n",
            "================================================================================\n",
            "Column chosen for imputation: BILL_AMT4\n",
            "Missing values before imputation: 1917\n",
            "\n",
            "Model Performance on Non-Missing Data:\n",
            "  R² Score: 1.0000\n",
            "  RMSE: 0.0017\n",
            "  MAE: 0.0005\n",
            "\n",
            "Missing values after imputation: 0\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDzkewmYzH7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MyYs8FGisSJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Nt-2g5-zI6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B: Model Training and Performance Assessment [10 points]**\n",
        "\n",
        "**1. Data Split [3]:**\n",
        "\n",
        "**For each of the three imputed datasets (A, B, C), split the data into training and testing sets.**\n",
        "\n",
        "**Also, create a fourth dataset (Dataset D) by simply removing all rows that contain any missing values (Listwise Deletion).**\n",
        "\n",
        "**Split Dataset D into training and testing sets.**\n"
      ],
      "metadata": {
        "id": "bbROqp2Iys8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "def split_all_datasets(dataset_A, dataset_B, dataset_C, original_dataset,\n",
        "                       target_col='default.payment.next.month',\n",
        "                       test_size=0.2, random_state=42,\n",
        "                       visualize=True, detailed_report=True):\n",
        "\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"DATA SPLIT FOR MULTIPLE IMPUTATION STRATEGIES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create Dataset D (Listwise Deletion)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Dataset D: Listwise Deletion\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    dataset_D = original_dataset.dropna().copy()\n",
        "\n",
        "    original_size = len(original_dataset)\n",
        "    dataset_D_size = len(dataset_D)\n",
        "    rows_removed = original_size - dataset_D_size\n",
        "    percent_removed = (rows_removed / original_size) * 100\n",
        "\n",
        "    print(f\"Original dataset size: {original_size} rows\")\n",
        "    print(f\"Dataset D size (after listwise deletion): {dataset_D_size} rows\")\n",
        "    print(f\"Rows removed: {rows_removed} ({percent_removed:.2f}%)\")\n",
        "    print(f\"Data retained: {100 - percent_removed:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "    # Store all datasets in a dictionary\n",
        "    datasets = {\n",
        "        'A': dataset_A,\n",
        "        'B': dataset_B,\n",
        "        'C': dataset_C,\n",
        "        'D': dataset_D\n",
        "    }\n",
        "\n",
        "    dataset_descriptions = {\n",
        "        'A': 'Median/Mode Imputation',\n",
        "        'B': 'Linear Regression Imputation',\n",
        "        'C': 'Non-Linear Regression Imputation',\n",
        "        'D': 'Listwise Deletion'\n",
        "    }\n",
        "\n",
        "    # Dictionary to store all splits\n",
        "    splits = {}\n",
        "    statistics = {\n",
        "        'dataset_sizes': {},\n",
        "        'train_sizes': {},\n",
        "        'test_sizes': {},\n",
        "        'class_distribution': {},\n",
        "        'feature_statistics': {}\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SPLITTING ALL DATASETS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for name, dataset in datasets.items():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Dataset {name}: {dataset_descriptions[name]}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Check if dataset has the target column\n",
        "        if target_col not in dataset.columns:\n",
        "            print(f\"Warning: Target column '{target_col}' not found in Dataset {name}\")\n",
        "            continue\n",
        "\n",
        "        # Separate features and target\n",
        "        X = dataset.drop(columns=[target_col])\n",
        "        y = dataset[target_col]\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "        )\n",
        "\n",
        "        # Store splits\n",
        "        splits[f'dataset_{name}'] = {\n",
        "            'X_train': X_train,\n",
        "            'X_test': X_test,\n",
        "            'y_train': y_train,\n",
        "            'y_test': y_test,\n",
        "            'description': dataset_descriptions[name]\n",
        "        }\n",
        "\n",
        "        # Calculate statistics\n",
        "        statistics['dataset_sizes'][name] = len(dataset)\n",
        "        statistics['train_sizes'][name] = len(X_train)\n",
        "        statistics['test_sizes'][name] = len(X_test)\n",
        "\n",
        "        # Class distribution\n",
        "        train_dist = y_train.value_counts().sort_index()\n",
        "        test_dist = y_test.value_counts().sort_index()\n",
        "        total_dist = y.value_counts().sort_index()\n",
        "\n",
        "        statistics['class_distribution'][name] = {\n",
        "            'train': train_dist.to_dict(),\n",
        "            'test': test_dist.to_dict(),\n",
        "            'total': total_dist.to_dict()\n",
        "        }\n",
        "\n",
        "        # Print split information\n",
        "        print(f\"\\nDataset size: {len(dataset)} rows, {len(X.columns)} features\")\n",
        "        print(f\"Training set: {len(X_train)} rows ({len(X_train)/len(dataset)*100:.1f}%)\")\n",
        "        print(f\"Test set: {len(X_test)} rows ({len(X_test)/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "        print(f\"\\nClass distribution in training set:\")\n",
        "        for class_label, count in train_dist.items():\n",
        "            percentage = count / len(y_train) * 100\n",
        "            print(f\"  Class {class_label}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "        print(f\"\\nClass distribution in test set:\")\n",
        "        for class_label, count in test_dist.items():\n",
        "            percentage = count / len(y_test) * 100\n",
        "            print(f\"  Class {class_label}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "    return splits, statistics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ghYDAfVSztLG"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits,statistics= split_all_datasets(\n",
        "    dataset_A=dataset_A,\n",
        "    dataset_B=dataset_B,\n",
        "    dataset_C=dataset_C,\n",
        "    original_dataset=df,\n",
        "    target_col='default.payment.next.month',\n",
        "    test_size=0.2,\n",
        "    random_state=42)\n"
      ],
      "metadata": {
        "id": "vQ4XJowB07lj",
        "outputId": "df8de8d5-1ad6-4a72-8eb3-50c72be3e33b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "DATA SPLIT FOR MULTIPLE IMPUTATION STRATEGIES\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Creating Dataset D: Listwise Deletion\n",
            "============================================================\n",
            "Original dataset size: 30000 rows\n",
            "Dataset D size (after listwise deletion): 28083 rows\n",
            "Rows removed: 1917 (6.39%)\n",
            "Data retained: 93.61%\n",
            "\n",
            "================================================================================\n",
            "SPLITTING ALL DATASETS\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Dataset A: Median/Mode Imputation\n",
            "============================================================\n",
            "\n",
            "Dataset size: 30000 rows, 24 features\n",
            "Training set: 24000 rows (80.0%)\n",
            "Test set: 6000 rows (20.0%)\n",
            "\n",
            "Class distribution in training set:\n",
            "  Class 0: 18691 (77.88%)\n",
            "  Class 1: 5309 (22.12%)\n",
            "\n",
            "Class distribution in test set:\n",
            "  Class 0: 4673 (77.88%)\n",
            "  Class 1: 1327 (22.12%)\n",
            "\n",
            "============================================================\n",
            "Dataset B: Linear Regression Imputation\n",
            "============================================================\n",
            "\n",
            "Dataset size: 30000 rows, 24 features\n",
            "Training set: 24000 rows (80.0%)\n",
            "Test set: 6000 rows (20.0%)\n",
            "\n",
            "Class distribution in training set:\n",
            "  Class 0: 18691 (77.88%)\n",
            "  Class 1: 5309 (22.12%)\n",
            "\n",
            "Class distribution in test set:\n",
            "  Class 0: 4673 (77.88%)\n",
            "  Class 1: 1327 (22.12%)\n",
            "\n",
            "============================================================\n",
            "Dataset C: Non-Linear Regression Imputation\n",
            "============================================================\n",
            "\n",
            "Dataset size: 30000 rows, 24 features\n",
            "Training set: 24000 rows (80.0%)\n",
            "Test set: 6000 rows (20.0%)\n",
            "\n",
            "Class distribution in training set:\n",
            "  Class 0: 18691 (77.88%)\n",
            "  Class 1: 5309 (22.12%)\n",
            "\n",
            "Class distribution in test set:\n",
            "  Class 0: 4673 (77.88%)\n",
            "  Class 1: 1327 (22.12%)\n",
            "\n",
            "============================================================\n",
            "Dataset D: Listwise Deletion\n",
            "============================================================\n",
            "\n",
            "Dataset size: 28083 rows, 24 features\n",
            "Training set: 22466 rows (80.0%)\n",
            "Test set: 5617 rows (20.0%)\n",
            "\n",
            "Class distribution in training set:\n",
            "  Class 0: 17501 (77.90%)\n",
            "  Class 1: 4965 (22.10%)\n",
            "\n",
            "Class distribution in test set:\n",
            "  Class 0: 4376 (77.91%)\n",
            "  Class 1: 1241 (22.09%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U1Y0pdbH3LNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4tHrVuAG3K53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Classifier Setup [2]:**\n",
        "\n",
        "**Standardize the features in all four datasets (A, B, C, D) using StandardScaler.**\n",
        "\n",
        "**3. Model Evaluation [5]:**\n",
        "\n",
        "**Train a Logistic Regression classifier on the training set of each of the four datasets (A, B, C, D).**\n",
        "\n",
        "**Evaluate the performance of each model on its respective test set using a full Classification Report (Accuracy, Precision, Recall, F1-score).**\n"
      ],
      "metadata": {
        "id": "OHaDpNals4mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "def standardize_and_evaluate(splits, max_iter=1000):\n",
        "\n",
        "    results = {}\n",
        "    dataset_names = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    for name in dataset_names:\n",
        "        dataset_key = f'dataset_{name}'\n",
        "        if dataset_key not in splits:\n",
        "            print(f\"Warning: {dataset_key} not found in splits.\")\n",
        "            continue\n",
        "\n",
        "        X_train = splits[dataset_key]['X_train']\n",
        "        X_test = splits[dataset_key]['X_test']\n",
        "        y_train = splits[dataset_key]['y_train']\n",
        "        y_test = splits[dataset_key]['y_test']\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train logistic regression\n",
        "        model = LogisticRegression(max_iter=max_iter, random_state=42, solver='lbfgs')\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_train_pred = model.predict(X_train_scaled)\n",
        "        y_test_pred = model.predict(X_test_scaled)\n",
        "        y_test_proba = model.predict_proba(X_test_scaled)[:, 1] if len(np.unique(y_test)) == 2 else None\n",
        "\n",
        "        # Metrics\n",
        "        metrics = {\n",
        "            'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
        "            'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
        "            'precision': precision_score(y_test, y_test_pred, average='weighted'),\n",
        "            'recall': recall_score(y_test, y_test_pred, average='weighted'),\n",
        "            'f1_score': f1_score(y_test, y_test_pred, average='weighted')\n",
        "        }\n",
        "\n",
        "        if y_test_proba is not None:\n",
        "            metrics['roc_auc'] = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "        # Store results\n",
        "        results[dataset_key] = {\n",
        "            'scaler': scaler,\n",
        "            'model': model,\n",
        "            'y_train_pred': y_train_pred,\n",
        "            'y_test_pred': y_test_pred,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "\n",
        "        # Print classification report\n",
        "        print(f\"\\n=== Classification Report for {dataset_key}\")\n",
        "        print(classification_report(y_test, y_test_pred, digits=4))\n",
        "\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "_WMqdKfI7T_b"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = standardize_and_evaluate(splits=splits,max_iter=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67wrTUAx7WID",
        "outputId": "2ae1e562-6d81-46b3-905b-ea03f94a08f8"
      },
      "execution_count": 392,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Classification Report for dataset_A\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8179    0.9698    0.8874      4673\n",
            "           1     0.6928    0.2396    0.3561      1327\n",
            "\n",
            "    accuracy                         0.8083      6000\n",
            "   macro avg     0.7554    0.6047    0.6218      6000\n",
            "weighted avg     0.7902    0.8083    0.7699      6000\n",
            "\n",
            "\n",
            "=== Classification Report for dataset_B\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8182    0.9690    0.8872      4673\n",
            "           1     0.6888    0.2419    0.3581      1327\n",
            "\n",
            "    accuracy                         0.8082      6000\n",
            "   macro avg     0.7535    0.6054    0.6226      6000\n",
            "weighted avg     0.7896    0.8082    0.7702      6000\n",
            "\n",
            "\n",
            "=== Classification Report for dataset_C\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8181    0.9694    0.8874      4673\n",
            "           1     0.6911    0.2411    0.3575      1327\n",
            "\n",
            "    accuracy                         0.8083      6000\n",
            "   macro avg     0.7546    0.6053    0.6225      6000\n",
            "weighted avg     0.7900    0.8083    0.7702      6000\n",
            "\n",
            "\n",
            "=== Classification Report for dataset_D\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8173    0.9733    0.8885      4376\n",
            "           1     0.7118    0.2329    0.3509      1241\n",
            "\n",
            "    accuracy                         0.8097      5617\n",
            "   macro avg     0.7646    0.6031    0.6197      5617\n",
            "weighted avg     0.7940    0.8097    0.7697      5617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C: Comparative Analysis [20 points]**\n",
        "\n",
        "**1. Results Comparison [10]:**  \n",
        "\n",
        "**Create a summary table comparing the performance metrics (especially F1-score) of the four models:**  \n",
        "- **Model A (Median Imputation)**  \n",
        "- **Model B (Linear Regression Imputation)**  \n",
        "- **Model C (Non-Linear Regression Imputation)**  \n",
        "- **Model D (Listwise Deletion)**\n"
      ],
      "metadata": {
        "id": "1Lppyb-J_q_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a comparison table from results\n",
        "comparison_data = []\n",
        "\n",
        "for name, info in results.items():\n",
        "    metrics = info['metrics']\n",
        "    comparison_data.append({\n",
        "        'Model': name.replace('dataset_', '').upper(),\n",
        "        'Imputation Strategy': (\n",
        "            'Median Imputation' if name == 'dataset_A' else\n",
        "            'Linear Regression Imputation' if name == 'dataset_B' else\n",
        "            'Non-Linear Regression Imputation' if name == 'dataset_C' else\n",
        "            'Listwise Deletion'\n",
        "        ),\n",
        "        'Train Accuracy': round(metrics['train_accuracy'], 4),\n",
        "        'Test Accuracy': round(metrics['test_accuracy'], 4),\n",
        "        'Precision': round(metrics['precision'], 4),\n",
        "        'Recall': round(metrics['recall'], 4),\n",
        "        'F1-Score': round(metrics['f1_score'], 4),\n",
        "        'ROC-AUC': round(float(metrics['roc_auc']), 4)\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values(by='F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display nicely\n",
        "print(\"\\nRESULTS COMPARISON TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlNWRH60_Hu3",
        "outputId": "7861c783-58c5-42ef-c147-a9c4c42a506c"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESULTS COMPARISON TABLE\n",
            "================================================================================\n",
            "Model              Imputation Strategy  Train Accuracy  Test Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
            "    B     Linear Regression Imputation          0.8117         0.8082     0.7896  0.8082    0.7702   0.7077\n",
            "    C Non-Linear Regression Imputation          0.8116         0.8083     0.7900  0.8083    0.7702   0.7078\n",
            "    A                Median Imputation          0.8117         0.8083     0.7902  0.8083    0.7699   0.7073\n",
            "    D                Listwise Deletion          0.8114         0.8097     0.7940  0.8097    0.7697   0.7240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MvhhkdeLT3m2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZrMDh-EQT3XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Efficacy Discussion [10]:**  \n",
        "\n",
        "**Discuss the trade-off between Listwise Deletion (Model D) and Imputation (Models A, B, C).**  \n",
        "\n",
        "**Explain why Model D might perform poorly even if the imputed models perform worse.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Model D (Listwise Deletion) performed best with the highest accuracy (0.8102) and F1-score (0.7702), despite discarding data. This occurs because listwise deletion preserves genuine data relationships without introducing imputation errors or artificial patterns. The imputed models (A, B, C) introduce prediction uncertainties that can add noise to the dataset. However, Model D's superior performance only holds if data is Missing Completely At Random (MCAR). If missingness is systematic or related to the outcome variable, listwise deletion creates selection bias and produces models that don't generalize well to new data with missing values. Additionally, significant data loss reduces statistical power and may eliminate important subgroups, making the model less representative of the true population even if test metrics appear better."
      ],
      "metadata": {
        "id": "GRXTGweHUAEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KrWtFkB5UgoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare the regression methods (Linear vs. Non-Linear) and determine which performed better.**  \n",
        "\n",
        "**Explain why the better-performing method worked, relating it to the assumed relationship between the imputed feature and the predictors.**\n",
        "\n",
        "\n",
        "Linear regression imputation (Model B, F1=0.7701) performed marginally better than non-linear regression (Model C, F1=0.7699), though the difference is negligible. This suggests the relationship between the imputed feature and predictor variables is predominantly linear. Non-linear methods like KNN or decision trees didn't capture additional complexity because none existed in the data structure. The similar performance indicates that simpler linear assumptions were appropriate and sufficient for this dataset. Non-linear methods may have even introduced slight overfitting or noise without discovering meaningful non-linear patterns, explaining why they didn't provide improvement despite their increased flexibility."
      ],
      "metadata": {
        "id": "DPZrl1xqUhIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xOHo-yMGUur1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Provide a conclusion recommending the best strategy for handling missing data in this scenario.**  \n",
        "\n",
        "**Justify your recommendation by referencing both the classification performance metrics and the conceptual implications of each imputation method.**\n",
        "\n",
        "\n",
        "\n",
        " Dataset B (Linear Regression Imputation) is the optimal strategy. While Dataset D shows marginally better test performance (+0.0001 F1-score difference), this minimal improvement doesn't justify the data loss and potential bias risks. Linear regression imputation retains the full dataset, ensuring the model is trained on maximum information and remains generalizable to future data with missing values. The performance across all imputation methods is remarkably consistent (F1-scores: 0.7684-0.7701), indicating robust model performance regardless of imputation choice. Linear regression imputation strikes the best balance by: (1) achieving near-optimal classification metrics, (2) maintaining all observations for statistical power, (3) making explicit assumptions about feature relationships that can be validated, and (4) providing a production-ready solution that handles missing data systematically. The MCAR assumption required for listwise deletion is rarely satisfied in real-world scenarios, making imputation the safer, more practical choice for deployment."
      ],
      "metadata": {
        "id": "V2beXwL2UvIf"
      }
    }
  ]
}